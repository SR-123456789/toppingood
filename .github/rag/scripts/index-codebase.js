const fs = require('fs-extra');
const path = require('path');
const glob = require('glob');
const { OpenAI } = require('openai');
require('dotenv').config();

/**
 * ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚’è‡ªå‹•ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
 */
class CodebaseIndexer {
  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    });
    this.projectRoot = path.join(__dirname, '../../..');
    this.ragRoot = path.join(__dirname, '..');
    this.config = this.loadConfig();
  }

  loadConfig() {
    const configPath = path.join(this.ragRoot, 'config', 'rag.json');
    const fileConfigPath = path.join(this.ragRoot, 'config', 'files.json');
    
    return {
      rag: require(configPath),
      files: require(fileConfigPath)
    };
  }

  async indexCodebase() {
    console.log('ğŸ“š ToppifyGOã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ä¸­...');

    try {
      // 1. ãƒ•ã‚¡ã‚¤ãƒ«åé›†
      const files = await this.collectFiles();
      console.log(`ğŸ“ ${files.length}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹`);

      // 2. ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ»ãƒãƒ£ãƒ³ã‚¯åŒ–
      const chunks = await this.processFiles(files);
      console.log(`ğŸ“ ${chunks.length}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆ`);

      // 3. åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
      const embeddings = await this.generateEmbeddings(chunks);
      console.log(`ğŸ§  ${embeddings.length}å€‹ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ`);

      // 4. ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«ä¿å­˜
      await this.saveToVectorStore(chunks, embeddings);
      console.log('ğŸ’¾ ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«ä¿å­˜å®Œäº†');

      // 5. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
      await this.saveMetadata(files, chunks);
      console.log('ğŸ“Š ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†');

      console.log('ğŸ‰ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–å®Œäº†ï¼');

    } catch (error) {
      console.error('âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã‚¨ãƒ©ãƒ¼:', error);
      process.exit(1);
    }
  }

  async collectFiles() {
    const { supportedExtensions, excludePatterns } = this.config.files;
    
    const patterns = supportedExtensions.map(ext => `**/*${ext}`);
    const excludeGlobs = excludePatterns.map(pattern => `**/${pattern}/**`);

    const files = [];
    for (const pattern of patterns) {
      const matches = glob.sync(pattern, {
        cwd: this.projectRoot,
        ignore: excludeGlobs,
        absolute: true
      });
      files.push(...matches);
    }

    return [...new Set(files)]; // é‡è¤‡é™¤å»
  }

  async processFiles(files) {
    const chunks = [];
    
    for (const file of files) {
      try {
        const content = await fs.readFile(file, 'utf8');
        const relativePath = path.relative(this.projectRoot, file);
        
        // ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®å‡¦ç†
        const fileChunks = await this.chunkFile(content, relativePath);
        chunks.push(...fileChunks);
        
      } catch (error) {
        console.warn(`âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: ${file}`, error.message);
      }
    }

    return chunks;
  }

  async chunkFile(content, filePath) {
    const { chunkSize, chunkOverlap } = this.config.rag.embedding;
    const ext = path.extname(filePath);
    
    // ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®å‰å‡¦ç†
    let processedContent = content;
    if (['.ts', '.js', '.tsx', '.jsx'].includes(ext)) {
      processedContent = this.preprocessCode(content, filePath);
    } else if (ext === '.md') {
      processedContent = this.preprocessMarkdown(content, filePath);
    }

    // ãƒãƒ£ãƒ³ã‚¯åŒ–
    const chunks = [];
    const lines = processedContent.split('\n');
    let currentChunk = '';
    let currentSize = 0;

    for (const line of lines) {
      if (currentSize + line.length > chunkSize && currentChunk.length > 0) {
        chunks.push({
          content: currentChunk.trim(),
          metadata: {
            filePath,
            type: this.getFileType(filePath),
            chunkIndex: chunks.length,
            lines: currentChunk.split('\n').length
          }
        });
        
        // ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å‡¦ç†
        const overlapLines = currentChunk.split('\n').slice(-Math.floor(chunkOverlap / 50));
        currentChunk = overlapLines.join('\n') + '\n' + line;
        currentSize = currentChunk.length;
      } else {
        currentChunk += (currentChunk ? '\n' : '') + line;
        currentSize += line.length + 1;
      }
    }

    if (currentChunk.trim()) {
      chunks.push({
        content: currentChunk.trim(),
        metadata: {
          filePath,
          type: this.getFileType(filePath),
          chunkIndex: chunks.length,
          lines: currentChunk.split('\n').length
        }
      });
    }

    return chunks;
  }

  preprocessCode(content, filePath) {
    // ã‚³ãƒ¼ãƒ‰ç”¨ã®å‰å‡¦ç†
    return `// File: ${filePath}\n${content}`;
  }

  preprocessMarkdown(content, filePath) {
    // Markdownç”¨ã®å‰å‡¦ç†
    return `# Document: ${filePath}\n${content}`;
  }

  getFileType(filePath) {
    const ext = path.extname(filePath);
    const typeMap = {
      '.ts': 'typescript',
      '.js': 'javascript', 
      '.tsx': 'react-typescript',
      '.jsx': 'react-javascript',
      '.sql': 'sql',
      '.py': 'python',
      '.md': 'markdown',
      '.txt': 'text',
      '.json': 'json',
      '.yaml': 'yaml',
      '.toml': 'toml'
    };
    return typeMap[ext] || 'unknown';
  }

  async generateEmbeddings(chunks) {
    const { model } = this.config.rag.embedding;
    const batchSize = 100;
    const embeddings = [];

    for (let i = 0; i < chunks.length; i += batchSize) {
      const batch = chunks.slice(i, i + batchSize);
      const texts = batch.map(chunk => chunk.content);
      
      try {
        const response = await this.openai.embeddings.create({
          model,
          input: texts
        });
        
        const batchEmbeddings = response.data.map(item => item.embedding);
        embeddings.push(...batchEmbeddings);
        
        console.log(`ğŸ§  åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: ${Math.min(i + batchSize, chunks.length)}/${chunks.length}`);
        
      } catch (error) {
        console.error(`âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼ (batch ${i}-${i + batchSize}):`, error);
        throw error;
      }
    }

    return embeddings;
  }

  async saveToVectorStore(chunks, embeddings) {
    // JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆå¾Œã§Pinecone/Supabaseã«ç§»è¡Œå¯èƒ½ï¼‰
    const vectorData = chunks.map((chunk, index) => ({
      id: `chunk_${index}`,
      embedding: embeddings[index],
      metadata: chunk.metadata,
      content: chunk.content
    }));

    const vectorPath = path.join(this.ragRoot, 'data', 'vectors', 'embeddings.json');
    await fs.writeJson(vectorPath, vectorData, { spaces: 2 });
  }

  async saveMetadata(files, chunks) {
    const metadata = {
      indexedAt: new Date().toISOString(),
      totalFiles: files.length,
      totalChunks: chunks.length,
      fileTypes: this.getFileTypeStats(files),
      config: this.config
    };

    const metadataPath = path.join(this.ragRoot, 'data', 'metadata.json');
    await fs.writeJson(metadataPath, metadata, { spaces: 2 });
  }

  getFileTypeStats(files) {
    const stats = {};
    files.forEach(file => {
      const type = this.getFileType(file);
      stats[type] = (stats[type] || 0) + 1;
    });
    return stats;
  }
}

async function main() {
  const indexer = new CodebaseIndexer();
  await indexer.indexCodebase();
}

if (require.main === module) {
  main();
}

module.exports = { CodebaseIndexer };
